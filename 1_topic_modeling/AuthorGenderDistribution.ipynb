{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gender-guesser in /home/ajanco/anaconda3/lib/python3.7/site-packages (0.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gender-guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()\n",
    "d.get_gender(u\"Igal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bc40d26b1973>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from lxml import etree\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Item:\n",
    "    id: int = None\n",
    "    author: str = None\n",
    "    journal_title: str = None\n",
    "    book_title: str = None\n",
    "    book_chapter: str = None\n",
    "    article_title: str = None\n",
    "    year: str =None\n",
    "    file: str = None\n",
    "    collection: str = None\n",
    "    jstor_url: str = None\n",
    "    ngrams: List[tuple] = field(default_factory=list)\n",
    "    \n",
    "    #list of all tokens \n",
    "    def tokens(self):\n",
    "        tokens = [a[0] for a in self.ngrams]\n",
    "        return tokens\n",
    "        \n",
    "    #frequency for a specific token\n",
    "    def token_freq(self, token:str):\n",
    "        a = [a for a in self.ngrams if a[0] == token]\n",
    "        return int(a[0][1])\n",
    "\n",
    "    #frequencies for a list of token\n",
    "    def token_list(self, tokens:list):\n",
    "        return [a for a in self.ngrams if a[0] in tokens]\n",
    "           \n",
    "    def hatebase_terms(self):\n",
    "        intersection = set([a[0].lower() for a in self.ngrams]).intersection(set(hatebase))\n",
    "        return [a for a in self.ngrams if a[0].lower() in intersection] \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_items(path):\n",
    "    current_dir = Path(path)\n",
    "    return_items = []\n",
    "    if (current_dir / 'metadata').exists():\n",
    "        count = 0 \n",
    "        for file in tqdm((current_dir / 'metadata').iterdir(), \n",
    "                         total=len(list((current_dir / 'metadata').iterdir()))):\n",
    "            item = Item()\n",
    "            item.id = count\n",
    "            count += 1\n",
    "            item.file = str(file.name)\n",
    "            item.collection = str(current_dir).split('/')[-1]\n",
    "            \n",
    "            \n",
    "            root = etree.fromstring(file.read_bytes())\n",
    "            try:\n",
    "                item.jstor_url = root.xpath('//self-uri')[0].attrib['{http://www.w3.org/1999/xlink}href']\n",
    "            except IndexError:\n",
    "                continue\n",
    "            if 'article' in file.name:\n",
    "                try:\n",
    "                    item.journal_title = root.xpath('//journal-title')[0].text\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                if root.xpath('//copyright-year'):\n",
    "                    item.year = root.xpath('//copyright-year')[0].text\n",
    "                if root.xpath('//string-name'):\n",
    "                    item.author = root.xpath('//string-name')[0].text #only first author\n",
    "                if root.xpath('//article-title'): #TODO not working, no results\n",
    "                    article_title = root.xpath('//article-title')[0].text \n",
    "            if 'book' in file.name:\n",
    "                if root.xpath('//book-title'):\n",
    "                    item.book_title = root.xpath('//book-title')[0].text\n",
    "                book_part_id = file.stem.split('_')[-1]\n",
    "                i = int(book_part_id.split('.')[-1])\n",
    "                try:\n",
    "                    item.book_chapter = root.xpath('//title')[i].text   \n",
    "                except IndexError:\n",
    "                    pass\n",
    "            \n",
    "            sub_dirs = ['ngram1','ngram2','ngram3']\n",
    "            for sub_dir in sub_dirs:\n",
    "                #book-chapter-10.2307_j.ctt1pc5dgp.4.xml maps to \n",
    "                #book-chapter-10.2307_j.ctt1pc5dgp.4-ngram1.txt\n",
    "                txt_file = str(file.stem)+'-'+sub_dir+'.txt'\n",
    "                ngram_file = (current_dir / sub_dir / txt_file )\n",
    "                if ngram_file.exists():\n",
    "                    text = ngram_file.read_text()\n",
    "                    for line in text.split('\\n'):\n",
    "                        word=line.split('\\t')[0]\n",
    "                        freq=line.split('\\t')[-1]\n",
    "                        item.ngrams.append((word,freq))\n",
    "            return_items.append(item)\n",
    "            \n",
    "        return return_items\n",
    "                \n",
    "                \n",
    "    \n",
    "    else:\n",
    "        print('Missing metadata directory')\n",
    "        raise SystemExit(0)\n",
    "        \n",
    "    \n",
    "directories = ['/home/ajanco/projects/slavic_review/slavic_review_data/SlavicStudiesCluster1991to2020'] #,'/home/ajanco/projects/slavic_review/slavic_review_data/AfricanAmericanStudiesCluster1985to2020']\n",
    "main_items = []\n",
    "for directory in directories:\n",
    "    main_items.extend(make_items(directory))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERTopic",
   "language": "python",
   "name": "bertopic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
